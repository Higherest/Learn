### 偏导

-   对于神经网络这个多元函数，每次只需要关注其中一个自变量比如 $w$ 即可。
    因为一次训练神经网络的迭代中，也只会（有先后顺序地）修改损失函数的每个自变量$w$ 和 $b$，而不是同时修改多个自变量。
    即**只考虑其中一个自变量该如何修改，使得损失函数的值更小**
    在数学领域，就是要求损失函数对这个自变量的偏导

### 学习速率

-   仅用导数的正负来指导自变量的变化方向，而不用导数的绝对值大小。
-   使用一个**学习速率** $\eta$ 来控制训练速度（每次训练迭代时修正权重、偏置的步长），
    学习速率不应太大或太小。太大则难以收敛（可以让学习速率与误差值大小成正比，误差越小，修正的步长越小）。太小则容易陷入局部最优（可以

### 更新

-   使用学习速率 $\eta$ 作为大小，偏导的正负作为正负，更新权重和偏置。以权重 $w$ 为例
    $$\displaystyle w_1 = w_1 - \eta \frac{\partial e}{\partial w_1}$$

