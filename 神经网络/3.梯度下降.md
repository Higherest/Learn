### 误差

-   由若干样本$x_1,x_2,...,x_n$作为**输入**，经过整个神经网络这个黑箱函数，得到**输出** $y_1,y_2,...,y_n$（最后的输出层有$n$个神经元）
-   输出 $y$ 与**目标值** $t$ 之间存在误差$E$，称为**损失函数**或**代价函数**，误差可以简单地用方差来表示。$E = \displaystyle\frac{(t_1 - y_1)^2 + ... + (t_n - y_n)^2}{n}$

### 损失函数

-   损失函数描述的是**权重**和**偏置**会对产生多少误差，因此损失函数的因变量是相关的所有**权重** $w$ 和**偏置** $b$ （而不是$y$），因变量当然就是误差了，损失函数可表示为$e(w_1,...,w_n,b_1,...,b_n)$
-   训练神经网络的目的就是**不断调整权重和偏置，使得误差不断变小**。既然 $w$ 和 $b$ 是损失函数的自变量，问题就变成了求（当这些自变量$w$ 和 $b$ 为多少时）这个损失函数才有最小值

### 降低误差

-   要求一个多元函数的最大值，可以对这个多元函数求多元微分？但是很困难。况且没法让计算机解方程
-   转换成另一种思路：不直接求出损失函数有最小值时对应的自变量 $w$ 和 $b$，而是将它们稍微朝着损失函数变小的方向移动。
    以一元函数为例，每一点都对应一个斜率即导数，表示当自变量增大时，函数值增大还是减小（以及变化的大小）。
    仅用导数的正负来指导自变量的变化方向，而不用导数的绝对值大小（采用学习速率）。
    为了让函数值变小，导数为负，则增大自变量；导数为正，则减小自变量。即**自变量的变化方向为导数的反方向**